'''
1.向量的内积是什么？有什么几何意义？
2.矩阵与其自身转置的乘积，得到的结果有什么意义？
'''

'''
键值对注意力
注意力机制的原始形态：
Attention（Q，K，V）= softmax（Q * KT/sqrt（dk））V
本质上为
Softmax（X * XT）* X


X * XT的意义：
矩阵可以看作由向量组成，矩阵的转置为互换行列。
矩阵与其自身的转置相乘就相当于自身与其他向量计算 内积

向量的内积的几何意义代表什么？
表示了两个向量的夹角，表征一个向量在另一个向量上的投影

'''




















